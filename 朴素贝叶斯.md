### 朴素贝叶斯

#### 准备数据：从文本中构建词向量

程序1：

```python
def loadDataSet():
    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not
    return postingList,classVec
                 
def createVocabList(dataSet):
    vocabSet = set([])  #create empty set
    for document in dataSet:
        vocabSet = vocabSet | set(document) #union of the two sets
    return list(vocabSet)

def setOfWords2Vec(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] = 1
        else: print "the word: %s is not in my Vocabulary!" % word
    return returnVec
```

​	第一个函数loadDataSet（）创建了一些实验样本。该函数返回的第一个变量是进行词条切分后的文档集合，第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。

​	下一个函数createVocabList()会创建一个包含在所有文档中出现的不重复词的列表，为此使用了Python的set数据类型。将词条列表输给set构造函数，set就会返回一个不重复的列表。

​	获得词汇表后使用函数setOfWords2Vec()，该函数的输入参数为词汇表及某个文档，输出时文档向量，向量的每一个元素为1或0，分别表示词汇中的单词在输入文档中是否出现。

​	查看函数执行效果，保存bayes.py文件，然后在Python提示符下输入：

```cmd
>>> import bayes
>>> listOPosts,listClasses = bayes.loadDataSet()
>>> myVocabList = bayes.createVocabList(listOPosts)
>>> myVocabList
['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']

>>> bayes.setOfWords2Vec( myVocabList, listOPosts[0])
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]
>>> bayes.setOfWords2Vec( myVocabList, listOPosts[3])
[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  

```

#### 训练算法：从词向量计算概率

程序2：

```python
def trainNB0(trainMatrix,trainCategory):
    numTrainDocs = len(trainMatrix)
    numWords = len(trainMatrix[0])
    pAbusive = sum(trainCategory)/float(numTrainDocs)
    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() 
    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0
    for i in range(numTrainDocs):
        if trainCategory[i] == 1:
            p1Num += trainMatrix[i]
            p1Denom += sum(trainMatrix[i])
        else:
            p0Num += trainMatrix[i]
            p0Denom += sum(trainMatrix[i])
    p1Vect = log(p1Num/p1Denom)          #change to log()
    p0Vect = log(p0Num/p0Denom)          #change to log()
    return p0Vect,p1Vect,pAbusive
```

​	代码函数中的输入参数为文档矩阵trainMatrix，以及由每篇文档类别标签所构成的向量trainCategory。

​	查看函数执行效果，将上述程序中的代码添加到bayes.py文件中，然后在Python提示符下输入：

```cmd
>>> from numpy import *
>>> reload(bayes)
<module 'bayes' from 'bayes.py'>
>>> listOPosts,listClasses = bayes.loadDataSet()
>>> myVocabList = bayes.createVocabList(listOPosts)
>>> trainMat = []
>>> for postinDoc in listOPosts:
...trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc)
...
```

​	该for循环使用词向量来填充trainMat列表。下面给出属于侮辱性文档的概率以及两个类别的概率向量。

```cmd
>>> p0V,p1V,pAb = bayes.trainNB0(trainMat,listClasses)
```

​	接下来看这些变量的内部值：

```cmd
>>> pAb
0.5
```

这就是任意文档属于侮辱性文档的概率。

#### 测量算法：根据现实情况修改分类器

​	利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算p(w<sub>0</sub>|1)p(w<sub>1</sub>|1)p(w<sub>2</sub>|1)。如果其中一个概率值为0，那么最后的乘积也为0.为降低这种影响，可以将所有词出现数初始化为1，并将分母初始化为2.

​	在文本编辑器中打开bayes.py文件，并将trainNB0（）的第4行和第5行修改为：

```python 
p0Num = ones(numWords);p1Num = ones(numWords)
p0Denom = 2.0;p1Denom = 2.0
```

​	另一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。一种解决办法是对乘积取自然对数。在代数中有 ln(a*b) = ln(a)+ln(b)，于是**通过求对数可以避免下溢出或者浮点数舍入导致的错误**。同时，采用自然对数进行处理不会有任何损失。如下图所示，**函数f(x)和ln(f(x))的曲线**。检查着两条曲线，就会发现它们在相同的区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终的结果。通过修改return前的两行代码，将上述做法用到分类器中：​    

```
p1Vect = log(p1Num/p1Denom)
p0Vect = log(p0Num/p0Denom)
```

![](D:\My University\2019WinterVacation\ML Note\ML in Action\Chapter 1\f(x)和ln(f(x))的曲线.png)

现在已经准备好构建完整的分类器了。当使用NumPy向量处理功能时，这一切变得十分简单。代开文本编辑器，将下面的代码添加到bayes.py中：

程序3：

```python
def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):
    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult
    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)
    if p1 > p0:
        return 1
    else: 
        return 0

def testingNB():
    listOPosts,listClasses = loadDataSet()
    myVocabList = createVocabList(listOPosts)
    trainMat=[]
    for postinDoc in listOPosts:
        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))
    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))
    testEntry = ['love', 'my', 'dalmation']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
    testEntry = ['stupid', 'garbage']
    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))
    print testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb)
```

​	上述程序中有4个输入：要分类的向量vec2Classify以及使用函数trainNB0（）计算得到的三个概率。使用NumPy的数组来计算两个向量相乘的结果（这里的相乘是指对应元素相乘）。接下来将词汇表中所有词的对应值相加，然后将该值加到类别的对数概率上。最后，比较类别的概率返回大概率对应的类别标签。代码的第二个函数是便利函数，该函数封装所有的操作，以节省输入代码的时间。

​	接下来运行程序看看实际效果，将上述程序加到beyas.py文件中，在Python提示符下输入:

```cmd
>>> reload(bayes)
<module 'bayes' from 'bayes.pyc'>
>>> bayes.testingNB()
['love', 'my', 'dalmation'] classified as:  0
['stupid', 'garbage'] classified as:  1
```

#### 准备数据：文档词袋模型

程序4：

```python
def bagOfWords2VecMN(vocabList, inputSet):
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            returnVec[vocabList.index(word)] += 1
    return returnVec

```

​	现在分类器已经构建好了，下面就是利用该分类器解决问题。