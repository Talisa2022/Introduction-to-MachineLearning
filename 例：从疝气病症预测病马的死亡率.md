### 例：从疝气病症预测病马的死亡率

​	使用Logistic回归来预测患有疝病的马的存活问题。这里的数据包含368个样本和28个特征。疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的肠胃问题，其它问题也有可能引发马疝病。该数据集中包含了医院检测马疝病的一些指标，有的指标比较客观，有的指标难以测量，例如马的疼痛级别。

**步骤：**

（1）收集数据：给定数据文件；

（2）准备数据：用python解析文本文件并填充缺失值；

（3）分析数据：可视化并观察数据；

（4）训练算法：使用优化算法，找到最佳的分类回归系数；

（5）测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数；

（6）使用算法：实现一个简单的命令程序来收集马的症状并输出预测结果。

​	除了部分指标比较直观外，该数据还存在一个问题：数据集中有30%的值是缺失的。下面先介绍缺失数据的处理。

#### 准备数据：处理数据中的缺失值

​	数据中的缺失值处理是一个棘手的问题，在很多的数值处理问题中都会遇到。若是直接丢弃整个数据集显然是不合理的，因为数据的收集并不容易，而且有时候数据非常昂贵。所以，必须采取一些方法来解决这个问题。

下面给出一些可供选择的做法：

1. 使用可用特征的均值来填充缺失值；

2. 使用特殊值来填充缺失值，如-1、0或者1；

3. 忽略有缺失值的样本；

4. 使用相似样本的均值填充缺失值；

5. 使用另外的机器学习算法预测缺失值；

   ​	现在，我们要对数据集进行预处理，使其可以顺利的使用分类算法。在预处理的过程中，我们要做两件事：

   （1）所有的缺失值必须用一个实数值来替换，因为我们是用的Numpy数据类型不允许包含缺失值。这里选择0来替换缺失值，恰好能用于Logistic回归（在更新时不会影响系数的值）；

   （2）如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征数据不同，很难确定采用某个合适的值来替换。采用Logistic回归进行分类时该做法是合理的，而如果使用类似KNN的方法就可能不太可行。

   #### 测试算法：用Logistic回归进行分类

   ​	使用logistic回归方法需要把数据集上每个特征向量乘以最优化方法得来的回归系数，再乘以该乘积结果求和，最后输入到Sigmoid函数中即可。如果对应的Sigmoid值大于0.5就预测类别标签1，否则为0。

   ​	下面看看实际运行效果，打开文本编辑器并将下列代码添加到一个py文件中。

   程序1：

   ```python
   def classifyVector(inX, weights):
       prob = sigmoid(sum(inX*weights))
       if prob > 0.5: return 1.0
       else: return 0.0
   
   def colicTest():
       frTrain = open('horseColicTraining.txt'); frTest = open('horseColicTest.txt')
       trainingSet = []; trainingLabels = []
       for line in frTrain.readlines():
           currLine = line.strip().split('\t')
           lineArr =[]
           for i in range(21):
               lineArr.append(float(currLine[i]))
           trainingSet.append(lineArr)
           trainingLabels.append(float(currLine[21]))
       trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000)
       errorCount = 0; numTestVec = 0.0
       for line in frTest.readlines():
           numTestVec += 1.0
           currLine = line.strip().split('\t')
           lineArr =[]
           for i in range(21):
               lineArr.append(float(currLine[i]))
           if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]):
               errorCount += 1
       errorRate = (float(errorCount)/numTestVec)
       print "the error rate of this test is: %f" % errorRate
       return errorRate
   
   def multiTest():
       numTests = 10; errorSum=0.0
       for k in range(numTests):
           errorSum += colicTest()
       print "after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests))
           
   ```

   ​	程序中的第一个函数**classifyVector( )**，它以回归系数和特征向量作为输入来计算Sigmoid值。如果Sigmoid值大于0.5返回1，否则返回0。接下来是一个名为**colicTest()**的函数适用于打开测试集与训练集，并对数据进行格式化处理的函数。最后一个函数是**multiTest()**，其功能是调用colicTest()函数10次并求结果的平均值。

   ​	下面看一下实际运行效果，在Python提示符下输入：

   ```python
   >>>reload(logRegres)
   <module 'logRegres' from 'logRegres.pyc'>
   >>> logRegres.multiTest()
   logRegres.py:18: RuntimeWarning: overflow encountered in exp
     return 1.0/(1+exp(-inX))
   the error rate of this test is: 0.298507
   the error rate of this test is: 0.268657
   the error rate of this test is: 0.388060
   the error rate of this test is: 0.343284
   the error rate of this test is: 0.358209
   the error rate of this test is: 0.283582
   the error rate of this test is: 0.313433
   the error rate of this test is: 0.388060
   the error rate of this test is: 0.343284
   the error rate of this test is: 0.343284
   after 10 iterations the average error rate is: 0.332836
   ```

   ​	从上面的结果我们可以看到，10次迭代之后的平均错误率为35%。事实上，这个结果并不差，因为有30%的数据缺失。当然，可以通过调节迭代次数和改进随机梯度上升算法的步长来降低平均错误率。
